import warnings
from abc import ABC
from pathlib import Path

import lightning as L
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from lightning.pytorch.callbacks import (
    BatchSizeFinder,
    DeviceStatsMonitor,
    EarlyStopping,
    LearningRateMonitor,
    ModelCheckpoint,
    RichProgressBar,
    Timer,
)
from lightning.pytorch.callbacks import Callback as LightningCallback
from lightning.pytorch.loggers import CometLogger, MLFlowLogger
from matplotlib.colors import PowerNorm
from pydantic import BaseModel

from radionets.evaluation.contour import area_of_contour
from radionets.evaluation.utils import apply_symmetry, get_ifft
from radionets.plotting.utils import get_vmin_vmax, set_cbar


class Callbacks:
    @classmethod
    def get_callbacks(cls, train_config: BaseModel) -> list:
        default_callback = RichProgressBar()
        callbacks = [default_callback]

        if train_config.callbacks.model_checkpoint:
            model_checkpoint = ModelCheckpoint(
                **train_config.callbacks.model_checkpoint.model_dump()
            )
            callbacks.append(model_checkpoint)

        if train_config.callbacks.batch_size_finder:
            batch_size_finder = BatchSizeFinder(
                **train_config.callbacks.batch_size_finder.model_dump()
            )
            callbacks.append(batch_size_finder)

        if train_config.callbacks.early_stopping:
            early_stopping = EarlyStopping(
                **train_config.callbacks.early_stopping.model_dump()
            )
            callbacks.append(early_stopping)

        if train_config.callbacks.lr_monitor:
            lr_monitor = LearningRateMonitor(
                **train_config.callbacks.lr_monitor.model_dump()
            )
            callbacks.append(lr_monitor)

        if train_config.callbacks.device_stats_monitor:
            callbacks.append(DeviceStatsMonitor())

        if train_config.callbacks.timer:
            timer = Timer(**train_config.callbacks.timer.model_dump())
            callbacks.append(timer)

        if train_config.logging.comet_ml:
            callbacks.append(CometCallback(train_config))

        if train_config.logging.mlflow:
            callbacks.append(MLFlowCallback(train_config))

            if train_config.logging.codecarbon:
                callbacks.append(MLFlowCodeCarbonCallback(train_config))

            callbacks.append(SourceRatioCallback(train_config))

        return callbacks


class PlottingCallbackABC(ABC, LightningCallback):
    def __init__(self, train_config, *args, **kwargs):
        super().__init__()
        self.train_config = train_config
        self.amp_phase = train_config.model.amp_phase
        self.scale = train_config.logging.scale

        self.cached_batch = None

        data_types = ["Amplitude", "Phase"] if self.amp_phase else ["Real", "Imaginary"]
        results = [" Prediction", " Ground Truth"]
        self.pred_plot_titles = [t + r for r in results for t in data_types]

    def plot_val_pred(self, predictions, targets, current_epoch: int):
        self.fig, self.axs = plt.subplots(
            2, 2, figsize=(12, 8.5), layout="constrained", sharex=True, sharey=True
        )
        self.axs = self.axs.flatten()

        limits_0 = get_vmin_vmax(targets[0, 0])  # Limits for amp/real
        limits_1 = get_vmin_vmax(targets[0, 1])  # Limits for phase/imaginary

        im0 = self.axs[0].imshow(
            predictions[0, 0],
            cmap="radionets.PuOr",
            vmin=-limits_0,
            vmax=limits_0,
            origin="lower",
        )
        im1 = self.axs[1].imshow(
            predictions[0, 1],
            cmap="radionets.PuOr",
            vmin=-limits_1,
            vmax=limits_1,
            origin="lower",
        )
        im2 = self.axs[2].imshow(
            targets[0, 0],
            cmap="radionets.PuOr",
            vmin=-limits_0,
            vmax=limits_0,
            origin="lower",
        )
        im3 = self.axs[3].imshow(
            targets[0, 1],
            cmap="radionets.PuOr",
            vmin=-limits_1,
            vmax=limits_1,
            origin="lower",
        )

        for ax, im, title in zip(
            self.axs,
            [im0, im1, im2, im3],
            self.pred_plot_titles,
        ):
            set_cbar(self.fig, ax, im, title=title, phase="Phase" in title)

        self.axs[0].set(ylabel="Frequels")
        self.axs[2].set(xlabel="Frequels", ylabel="Frequels")
        self.axs[3].set(xlabel="Frequels")

    def plot_val_fft(self, predictions, targets, current_epoch):
        ifft_pred = get_ifft(
            predictions,
            amp_phase=self.amp_phase,
            scale=self.scale,
        )
        ifft_truth = get_ifft(targets, amp_phase=self.amp_phase, scale=self.scale)

        self.fig, self.axs = plt.subplots(1, 3, figsize=(16, 4.5), layout="constrained")

        im0 = self.axs[0].imshow(
            ifft_pred,
            norm=PowerNorm(0.25, vmax=ifft_truth.max()),
            cmap="inferno",
            origin="lower",
        )
        im1 = self.axs[1].imshow(
            ifft_truth,
            norm=PowerNorm(0.25),
            cmap="inferno",
            origin="lower",
        )

        limits = get_vmin_vmax(ifft_pred - ifft_truth)
        im2 = self.axs[2].imshow(
            ifft_pred - ifft_truth,
            cmap="radionets.PuOr",
            vmin=-limits,
            vmax=limits,
            origin="lower",
        )

        for ax, im, title in zip(
            self.axs,
            [im0, im1, im2],
            ["Prediction", "Truth", "Difference"],
        ):
            set_cbar(self.fig, ax, im, title="FFT " + title)

        self.axs[0].set(
            ylabel="Pixels",
            xlabel="Pixels",
        )
        self.axs[1].set_xlabel("Pixels")
        self.axs[2].set_xlabel("Pixels")

    def on_validation_epoch_end(self, trainer: L.Trainer, pl_module) -> None:
        """Log predictions at validation epoch end."""

        if self.cached_batch is None:
            val_dataloader = trainer.datamodule.val_dataloader()
            batch = next(iter(val_dataloader))

            # cache only one sample
            self.cached_batch = (
                batch[0][0][None, ...].cpu(),
                batch[1][0][None, ...].cpu(),
            )

        if (trainer.current_epoch + 1) % self.train_config.logging.plot_n_epochs == 0:
            batch = (
                self.cached_batch[0].to(pl_module.device),
                self.cached_batch[1].to(pl_module.device),
            )

            predictions = pl_module.predict_step(batch, batch_idx=0).cpu()
            targets = batch[1].cpu()

            # check if images are half or full
            if predictions.shape[-2] != predictions.shape[-1]:
                predictions = apply_symmetry(predictions)
                targets = apply_symmetry(targets)

            self.plot_val_pred(
                predictions,
                targets,
                current_epoch=trainer.current_epoch,
            )

            self.plot_val_fft(
                predictions,
                targets,
                current_epoch=trainer.current_epoch,
            )


class CometCallback(PlottingCallbackABC):
    def __init__(self, train_config, *args, **kwargs):
        super().__init__(train_config, *args, **kwargs)
        self.experiment = None

    def plot_val_pred(self, predictions, targets, current_epoch: int) -> None:
        super().plot_val_pred(predictions, targets, current_epoch)

        self.experiment.log_figure(
            figure=self.fig,
            figure_name=f"fourier_pred_{current_epoch}",
        )

        plt.close(self.fig)

    def plot_val_fft(self, predictions, targets, current_epoch: int) -> None:
        super().plot_val_fft(predictions, targets, current_epoch)

        self.experiment.log_figure(
            figure=self.fig,
            figure_name=f"fft_pred_{current_epoch}",
        )
        plt.close(self.fig)

    def on_validation_epoch_end(self, trainer: L.Trainer, pl_module) -> None:
        """Log predictions at validation epoch end."""
        if self.experiment is None:
            try:
                self.experiment = next(
                    logger.experiment
                    for logger in trainer.loggers
                    if isinstance(logger, CometLogger)
                )
            except StopIteration as e:
                raise ValueError(
                    f"Could not find a CometLogger instance in {trainer.loggers}."
                ) from e

        super().on_validation_epoch_end(trainer, pl_module)


class MLFlowCallback(PlottingCallbackABC):
    def __init__(self, train_config, *args, **kwargs):
        super().__init__(train_config, *args, **kwargs)

        self.experiment = None

    def plot_val_pred(self, predictions, targets, current_epoch: int) -> None:
        super().plot_val_pred(predictions, targets, current_epoch)

        artifact_file = str(self.base_dir) + f"/fourier_pred_{current_epoch:0>4}.png"

        self.experiment.log_figure(
            figure=self.fig,
            artifact_file=artifact_file,
            run_id=self.logger._run_id,
        )

        plt.close(self.fig)

    def plot_val_fft(self, predictions, targets, current_epoch: int) -> None:
        super().plot_val_fft(predictions, targets, current_epoch)

        artifact_file = str(self.base_dir) + f"/fft_pred_{current_epoch:0>4}.png"

        self.experiment.log_figure(
            figure=self.fig,
            artifact_file=artifact_file,
            run_id=self.logger._run_id,
        )
        plt.close(self.fig)

    def on_validation_epoch_end(self, trainer: L.Trainer, pl_module) -> None:
        """Log predictions at validation epoch end."""
        if self.experiment is None:
            try:
                self.logger = next(
                    logger
                    for logger in trainer.loggers
                    if isinstance(logger, MLFlowLogger)
                )
                self.experiment = self.logger.experiment

                self.base_dir = (
                    self.train_config.paths.model_path / f"mlflow/{self.logger._run_id}"
                )
                self.base_dir.mkdir(parents=True)

            except StopIteration as e:
                raise ValueError(
                    f"Could not find a MLFlowLogger instance in {trainer.loggers}."
                ) from e

        super().on_validation_epoch_end(trainer, pl_module)


class MLFlowCodeCarbonCallback(LightningCallback):
    def __init__(self, train_config, *args, **kwargs):
        self.train_config = train_config

        self.experiment = None

    def on_fit_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        try:
            self._log_metrics()
        except FileNotFoundError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)
        except KeyError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)

    def on_test_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        try:
            self._log_metrics()
        except FileNotFoundError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)
        except KeyError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)

    def on_predict_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        try:
            self._log_metrics()
        except FileNotFoundError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)
        except KeyError as e:
            warnings.warn(f"{e}. No emissions were logged.", stacklevel=2)

    def _set_up_experiment(self, trainer):
        try:
            self.logger = next(
                logger for logger in trainer.loggers if isinstance(logger, MLFlowLogger)
            )
            self.experiment = self.logger.experiment

        except StopIteration as e:
            raise ValueError(
                f"Could not find a MLFlowLogger instance in {trainer.loggers}."
            ) from e

    def _log_metrics(self):
        emission_file = Path(
            self.train_config.logging.codecarbon.output_dir + "/emissions.csv"
        )
        emission_data = pd.read_csv(emission_file).to_dict()

        eval_res = dict(
            running_time_total=emission_data["duration"][0],
            running_time=emission_data["duration"][0],
            power_draw_total=emission_data["energy_consumed"][0] * 3.6e6,
            power_draw=emission_data["energy_consumed"][0] * 3.6e6,
        )

        for key, val in eval_res.items():
            self.experiment.log_metric(
                key=key,
                value=val,
                run_id=self.logger._run_id,
            )

        # Remove file after logging all important metrics to mlflow.
        # This prevents codecarbon from creating 'emissions.csv_%d.bak'
        # files in the save directory
        if emission_file.is_file():
            emission_file.unlink()


class SourceRatioCallback(LightningCallback):
    def __init__(self, train_config, *args, **kwargs):
        self.train_config = train_config
        self.amp_phase = train_config.model.amp_phase

        self.experiment = None

    def on_fit_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        self._log_metrics(
            dataloader=trainer.datamodule.val_dataloader(),
            pl_module=pl_module,
        )

    def on_test_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        self._log_metrics(
            dataloader=trainer.datamodule.test_dataloader(),
            pl_module=pl_module,
        )

    def on_predict_end(self, trainer, pl_module):
        if self.experiment is None:
            self._set_up_experiment(trainer)

        self._log_metrics(
            dataloader=trainer.datamodule.predict_dataloader(),
            pl_module=pl_module,
        )

    def _set_up_experiment(self, trainer):
        try:
            self.logger = next(
                logger for logger in trainer.loggers if isinstance(logger, MLFlowLogger)
            )
            self.experiment = self.logger.experiment

        except StopIteration as e:
            raise ValueError(
                f"Could not find a MLFlowLogger instance in {trainer.loggers}."
            ) from e

    def _log_metrics(self, dataloader, pl_module):
        area = []
        for batch in dataloader:
            preds = pl_module.predict_step(batch[0], batch_idx=0).detach().cpu()
            targets = batch[1].detach().cpu()

            # check if images are half or full
            if preds.shape[-2] != preds.shape[-1]:
                preds = apply_symmetry(preds)
                targets = apply_symmetry(targets)

            ifft_preds = get_ifft(preds, amp_phase=self.amp_phase)
            ifft_targets = get_ifft(targets, amp_phase=self.amp_phase)

            print(ifft_preds.shape)

            area.extend(
                [
                    area_of_contour(ifft_pred, ifft_target)
                    for ifft_pred, ifft_target in zip(ifft_preds, ifft_targets)
                ]
            )

        self.experiment.log_metric(
            key="area_ratio",
            value=np.mean(area),
            run_id=self.logger._run_id,
        )
